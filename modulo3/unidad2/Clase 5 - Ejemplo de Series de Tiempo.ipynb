{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Clase 5 - Ejemplo de Series de Tiempo.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7cUqukj4_P_A"},"source":["![img](https://drive.google.com/uc?export=view&id=1kgX98Ziw9LzgBCT2BxChttfrTcBltLej)\n"]},{"cell_type":"markdown","metadata":{"id":"mgqhxY2yCx6i"},"source":["# MÓDULO 3. Unidad 2 - Clase 1\n","\n","---\n","\n","> Redes Neuronales Recurrentes.\n","\n","> Red Neuronal Recurrente Simple en Keras. Funcionamiento.\n","\n","> Ejemplo de Aplicación: Predicción del Precio de un Activo.\n","\n","> Otros Tipos Más Sofisticados de RNN"]},{"cell_type":"markdown","metadata":{"id":"QKoaRBY2KwGN"},"source":["# Redes Neuronales Recurrentes (RNN, Recurrent Neural Networks)\n","\n","Las **redes neuronales recurrentes** son un tipo particular de red neuronal que agrega recurrencia a la relación entre las neuronas de una capa. Esta recurrencia es lo que introduce una especie de \"memoria\" a la topología de la red.\n","\n","![img](https://drive.google.com/uc?export=view&id=14CVwzArMr-lbnICRt5HI-9K7GL80K10h)\n","\n","Las redes recurrentes son utilizadas para resolver problemas de predicción de secuencias, como por ejemplo problemas de predicción de series temporales, ya que le podemos pasar una cantidad $n$ de valores (o *steps*) para predecir el valor $n+1$ (próximo *step*). También se usan para predecir la próxima palabra, dada una secuencia de palabras. Les suena esto?\n","\n","Las aplicaciones y contenidos de esta notebook están basados en el siguiente post: https://machinelearningmastery.com/understanding-simple-recurrent-neural-networks-in-keras/"]},{"cell_type":"markdown","metadata":{"id":"HYCK4YMaSlKm"},"source":["# Red Neuronal Recurrente Simple en **Keras**\n","\n","Veamos cómo podemos crear una RNN usando Keras."]},{"cell_type":"markdown","metadata":{"id":"7J6nGyT1iVW_"},"source":["## Importaciones"]},{"cell_type":"code","metadata":{"id":"SvYIuHDvD_u3"},"source":["# Correr si es necesario, para instalar Yahoo Finance\n","!pip install yfinance"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3MSZPRfZiDJb"},"source":["import pandas as pd\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, SimpleRNN\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","import math\n","import matplotlib.pyplot as plt\n","import yfinance as yf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-nX8Db25EFOh"},"source":["import tensorflow as tf\n","import random as python_random\n","\n","# The below is necessary for starting Numpy generated random numbers\n","# in a well-defined initial state.\n","np.random.seed(123)\n","\n","# The below is necessary for starting core Python generated random numbers\n","# in a well-defined state.\n","python_random.seed(123)\n","\n","# The below set_seed() will make random number generation\n","# in the TensorFlow backend have a well-defined initial state.\n","# For further details, see:\n","# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n","tf.random.set_seed(123)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRszs6jWiYQ1"},"source":["## Red Neuronal Recurrente Simple\n","\n","Con la siguiente función, creamos un modelo usando la librería **Keras** que incluye una capa `SimpleRNN` y una capa `Dense`(densa) para aprender datos secuenciales."]},{"cell_type":"code","metadata":{"id":"NB9Bk2M-iEy8"},"source":["def create_RNN(hidden_units, dense_units, input_shape, activation):\n","    model = Sequential()\n","    model.add(SimpleRNN(hidden_units, input_shape=input_shape, \n","                        activation=activation[0]))\n","    model.add(Dense(units=dense_units, activation=activation[1]))\n","    model.compile(loss='mean_squared_error', optimizer='adam')\n","    return model\n","\n","demo_model = create_RNN(2, 1, (3,1), activation=['linear', 'linear'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8bLVJK6cQZlG"},"source":["El `input_shape` se establece en 3×1 y, para simplificar, se usa la activación lineal para ambas capas ($f(x) = x$).\n","\n","Entonces, si tenemos $m$ unidades ocultas (`hidden_units`, en este caso $m=2$), la red tiene la siguiente pinta:\n","\n","- Input: $x \\in R$\n","- Hidden unit (Unidades ocultas): $h \\in R^m$\n","- Weights o pesos para las input units: $w_x \\in R^m$\n","- Weights o pesos para las hidden units: $w_h \\in R^{mxm}$\n","- Bias o sesgos para las hidden units: $b_h \\in R^m$\n","- Weight o pesos para la *dense layer* (capa densa): $w_y \\in R^m$\n","- Bias o sesgo para la *dense layer* (capa densa): $b_y \\in R$\n","\n","Veamos estos pesos en nuestro modelo:"]},{"cell_type":"code","metadata":{"id":"tN0KggzmiEvH"},"source":["wx = demo_model.get_weights()[0]\n","wh = demo_model.get_weights()[1]\n","bh = demo_model.get_weights()[2]\n","wy = demo_model.get_weights()[3]\n","by = demo_model.get_weights()[4]\n","\n","print('wx = ', wx)\n","print('shape = ', wx.shape)\n","print()\n","print('wh = ', wh)\n","print('shape = ', wh.shape)\n","print()\n","print('bh = ', bh)\n","print('shape = ', bh.shape)\n","print()\n","print('wy =', wy)\n","print('shape = ', wy.shape)\n","print()\n","print('by = ', by)\n","print('shape = ', by.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dS9e5fICdQ39"},"source":["Podemos analizar la estructura de este modelo a partir de la siguiente imagen:\n","\n","![img](https://drive.google.com/uc?export=view&id=14BiPrb76ujujdP5mvqYlCJQYv70P7ulR)\n","\n","Entonces, vamos a darle como input al modelo 3 valores secuenciales y esperar que la red genere una salida.\n","\n","Se calcularán los valores de las unidades ocultas a los momentos de tiempo 1, 2 y 3. $h_0$ es inicializado con un vector de ceros. El output ($y_3$) es calculado utilizando $h_3$ y $w_y$. Veamos como funciona esto."]},{"cell_type":"code","metadata":{"id":"vH5wjt9JiEnl"},"source":["# Creamos un array con 3 valores\n","x = np.array([1, 2, 3])\n","# Hacemos un reshape del input para obtener: sample_size x time_steps x features \n","x_input = np.reshape(x,(1, 3, 1))\n","x_input"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bs79BJyqlcIq"},"source":["y_pred_model = demo_model.predict(x_input)\n","\n","m = 2\n","h0 = np.zeros(m)\n","h1 = np.dot(x[0], wx) + h0 + bh\n","h2 = np.dot(x[1], wx) + np.dot(h1,wh) + bh\n","h3 = np.dot(x[2], wx) + np.dot(h2,wh) + bh\n","o3 = np.dot(h3, wy) + by\n","\n","print('h0 = ', h0)\n","print('h1 = ', h1)\n","print('h2 = ', h2)\n","print('h3 = ', h3)\n","\n","print(\"Predicción de la red: \", y_pred_model)\n","print(\"Predicción de nuestro cálculo: \", o3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWM38qNU2IuN"},"source":["# Ejemplo de Aplicación: Predicción del Precio de un Activo.\n","\n","Veamos un caso de aplicación para tratar de predecir el precio de un activo. Para ello, seguiremos los siguientes pasos:\n","\n","1. Lectura del Dataset\n","2. División de los datos en train y test\n","3. Preparación del input al formato requerido por Keras\n","4. Creación y entrenamiento de un modelo RNN\n","5. Obtención de las predicciones de train y test y los RMSE\n","6. Visualización de los resultados"]},{"cell_type":"markdown","metadata":{"id":"91sqpPx_3Qku"},"source":["## Pasos 1 y 2: Lectura y División de los Datos en Train y Test\n","\n","La siguiente función, divide los datos en train y test, dada una serie de pandas con estructura temporal. Devuelve arrays de una dimensión para train y test.\n","\n","Antes de utilizar esta función, escalaremos los datos entre 0 y 1, usando `MinMaxScaler` de Scikit-Learn.\n"]},{"cell_type":"code","metadata":{"id":"j3zq3epblvBV"},"source":["# Parameter split_percent defines the ratio of training examples\n","def get_train_test(dataframe, split_percent=0.8):\n","    data = np.array(dataframe.values.astype('float32'))\n","    n = len(data)\n","    # Point for splitting data into train and test\n","    split = int(n*split_percent)\n","    train_data = data[range(split)]\n","    test_data = data[split:]\n","    return train_data, test_data, data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lYhrqRUp3Ftc"},"source":["Usando Yahoo Finance, podemos obtener el precio histórico de algún activo. En este caso, usaremos el precio de cierre de un año para Microsoft (ticker `MSFT`)."]},{"cell_type":"code","metadata":{"id":"mr6VZMgGD9vZ"},"source":["msft = yf.Ticker(\"MSFT\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lWoAfCXbEJQ5"},"source":["msft_close = msft.history(period=\"1y\")[['Close']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"biwbUAFyE8Ny"},"source":["import plotly.express as px\n","px.line(data_frame=msft_close, x=msft_close.index, y='Close')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pe5a9e5h6y7A"},"source":["scaler = MinMaxScaler(feature_range=(0, 1))\n","data = scaler.fit_transform(msft_close).flatten()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QiTib8HA66g_"},"source":["# scaler.inverse_transform(msft_close)\n","msft_close['Close'] = data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjJIaBkuBtnF"},"source":["train_data, test_data, data = get_train_test(msft_close)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pRY8PBDi3UFb"},"source":["## Paso 3: Hacemos Reshape de la Data para Keras\n","\n","En este paso, se preparan los datos para el entrenamiento del modelo en Keras. El *input array* debería tener la siguiente forma: `total_samples` x `time_steps` x `features`.\n","\n","Hay muchas maneras de preparar datos de series temporales para el entrenamiento. En este caso, vamos a crear un input con *time steps* no superpuestos, tal como se puede ver en la siguiente imagen para 2 `time_steps`. Este parámetro indica la cantidad de valores $t$ a utilizar para predecir el valor $t+1$.\n","\n","![img](https://drive.google.com/uc?export=view&id=14E-ny1p0lwvemV6WfSC1rYmIl4jZLXqw)\n","\n","La función `get_XY()` toma un array unidimensional como input y lo \n","convierte en los arrays de input X y target Y que necesitamos.\n","Probemos usando 10 `time_steps` (relacionado a la ciclicidad de los datos) y veamos qué ocurre."]},{"cell_type":"code","metadata":{"id":"lxItmz6n3Uvm"},"source":["# Prepare the input X and target Y\n","def get_XY(dat, time_steps):\n","    # Indices of target array\n","    Y_ind = np.arange(time_steps, len(dat), time_steps)\n","    Y = dat[Y_ind]\n","    # Prepare X\n","    rows_x = len(Y)\n","    X = dat[range(time_steps*rows_x)]\n","    X = np.reshape(X, (rows_x, time_steps, 1))    \n","    return X, Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nRQcc7KWBHWz"},"source":["time_steps = 10\n","trainX, trainY = get_XY(train_data, time_steps)\n","testX, testY = get_XY(test_data, time_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AwNYtmfR-_Tn"},"source":["trainX.shape, trainY.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ekaq5fSU3VB4"},"source":["## Paso 4: Crear un Modelo RNN y Entrenarlo"]},{"cell_type":"code","metadata":{"id":"n63FVf4MEUYP"},"source":["def create_RNN(hidden_units, dense_units, input_shape, activation):\n","    model = Sequential()\n","    model.add(SimpleRNN(hidden_units, input_shape=input_shape, \n","                        activation=activation[0]))\n","    model.add(Dense(units=dense_units, activation=activation[1]))\n","    model.compile(loss='mean_squared_error', optimizer='adam')\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hOIONktw3VVC"},"source":["model = create_RNN(hidden_units=3, dense_units=1, input_shape=(time_steps,1), \n","                   activation=['tanh', 'tanh'])\n","model.fit(trainX, trainY, epochs=10, batch_size=1, verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LU93eHRD3Vqf"},"source":["## Paso 5: Calcular la Raíz del Error Cuadrático Medio (RMSE)\n","\n","La función `print_error()` calcula la raíz el error cuadrático medio entre los valores reales y los predichos.\n"]},{"cell_type":"code","metadata":{"id":"UCNiN9V23V9D"},"source":["def print_error(trainY, testY, train_predict, test_predict):    \n","    # Error of predictions\n","    train_rmse = math.sqrt(mean_squared_error(trainY, train_predict))\n","    test_rmse = math.sqrt(mean_squared_error(testY, test_predict))\n","    # Print RMSE\n","    print('Train RMSE: %.3f RMSE' % (train_rmse))\n","    print('Test RMSE: %.3f RMSE' % (test_rmse))    \n","\n","# Obtener predicciones\n","train_predict = model.predict(trainX)\n","test_predict = model.predict(testX)\n","# Reescaladas\n","rescaled_train_predict = scaler.inverse_transform(model.predict(trainX))\n","rescaled_test_predict = scaler.inverse_transform(model.predict(testX))\n","\n","rescaled_trainY = scaler.inverse_transform(trainY)\n","rescaled_testY = scaler.inverse_transform(testY)\n","# Mean square error\n","print('Con Scaling')\n","print_error(trainY, testY, train_predict, test_predict)\n","print('Sin Scaling')\n","print_error(rescaled_trainY, rescaled_testY, rescaled_train_predict, rescaled_test_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W9hUBQ3n4cUW"},"source":["## Paso 6: Visualizar los Resultados\n","\n","La siguiente funcion grafica los valores reales de nuestra serie temporal y los predichos. La línea roja separa los datos de train y test."]},{"cell_type":"code","metadata":{"id":"wZx30f7_4cln"},"source":["# Plot the result\n","def plot_result(trainY, testY, train_predict, test_predict):\n","    actual = np.append(trainY, testY)\n","    predictions = np.append(train_predict, test_predict)\n","    rows = len(actual)\n","    plt.figure(figsize=(15, 6), dpi=80)\n","    plt.plot(range(rows), actual)\n","    plt.plot(range(rows), predictions)\n","    plt.axvline(x=len(trainY), color='r')\n","    plt.legend(['Actual', 'Predictions'])\n","    plt.xlabel('Observation number after given time steps')\n","    plt.ylabel('Sunspots scaled')\n","    plt.title('Actual and Predicted Values. The Red Line Separates The Training And Test Examples')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLvpSLWnA-Xq"},"source":["# plot_result(trainY, testY, train_predict, test_predict)\n","plot_result(rescaled_trainY, rescaled_testY, rescaled_train_predict, rescaled_test_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"boVW-rtgp_hy"},"source":["# Otros Tipos más Sofisticados de RNN\n","\n","Otro tipo de red recurrente, más sofisticado que la capa simple que vimos es la **LSTM** (**Long Short-Term Memory**), que viene a subsanar algunos inconvenientes de la red recurrente más simple (el problema de desvanecimiento del gradiente o *vanishing gradient* que afecta su performance).\n","\n","A diferencia de las redes recurrentes tradicionales, una red neuronal *LSTM* está mejor adaptada para aprender de la experiencia a clasificar, procesar y predecir series temporales cuando hay retardos o *lags* de tiempos muy prolongados de tamaño desconocido entre eventos relevantes.\n","\n","La habilidad de este algoritmo de tratar con retardos o *lags* muy prolongados tiene que ver con como hace la propagación hacia atrás del error (*Backpropagation*) de una forma constante.\n","\n","Para más información, pueden consultar el siguiente material:\n","\n","- https://www.altumintelligence.com/articles/a/Time-Series-Prediction-Using-LSTM-Deep-Neural-Networks\n","- https://machinelearningmastery.com/prepare-univariate-time-series-data-long-short-term-memory-networks/\n","- https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/7.%20Recurrent%20Neural%20Networks/7.1%20RNN%20and%20LSTM.ipynb\n","- https://towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7\n","- https://towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7"]}]}